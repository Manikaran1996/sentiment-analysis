{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from keras.layers import LSTM, Dense, Embedding, Masking\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import Sequential\n",
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "from keras.models import load_model\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_text = './imdbDataset/imdb_train_text.txt'\n",
    "train_file_label = './imdbDataset/imdb_train_labels.txt'\n",
    "test_file_text = './imdbDataset/imdb_test_text.txt'\n",
    "test_file_label = './imdbDataset/imdb_test_labels.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_reviews = []\n",
    "with open(train_file_text) as fin:\n",
    "    for l in fin:\n",
    "        training_reviews.append(l.replace('<br /><br />', \" \").strip(\"\\\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_reviews = []\n",
    "with open(test_file_text) as fin:\n",
    "    for l in fin:\n",
    "        test_reviews.append(l.replace('<br /><br />', \" \").strip(\"\\\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_labels = []\n",
    "with open(train_file_label) as fin:\n",
    "    for l in fin:\n",
    "        training_labels.append(int(l.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = []\n",
    "with open(test_file_label) as fin:\n",
    "    for l in fin:\n",
    "        test_labels.append(int(l.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataprep(reviews):\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    eng_stopwords = stopwords.words('english')\n",
    "    # word-tokenize\n",
    "    words_in_reviews = [review.split() for review in reviews]\n",
    "    print(\"tokenization done..\")\n",
    "    # removing punctuation\n",
    "    words_in_reviews = [[word.translate(table) for word in sent] for sent in words_in_reviews]\n",
    "    print(\"punctuations removed...\")\n",
    "    # removing stop-words\n",
    "    words_in_reviews = [[w.lower() for w in review_words if w.lower() not in eng_stopwords and len(w) > 2] for review_words in words_in_reviews]\n",
    "    print(\"stopwords removed...\")\n",
    "    # stemming\n",
    "    #stemmer = PorterStemmer()\n",
    "    #stemmed_reviews = [[stemmer.stem(w) for w in words] for words in words_in_reviews]\n",
    "    #print(\"stemming done...\")\n",
    "    #return stemmed_reviews\n",
    "    return words_in_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenization done..\n",
      "punctuations removed...\n",
      "stopwords removed...\n"
     ]
    }
   ],
   "source": [
    "test_processed_reviews = dataprep(test_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenization done..\n",
      "punctuations removed...\n",
      "stopwords removed...\n"
     ]
    }
   ],
   "source": [
    "train_processed_reviews = dataprep(training_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vecModel = word2vec.Word2Vec(train_processed_reviews + test_processed_reviews, iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vecModel.corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5906919"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vecModel.corpus_total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('asleep', 0.5823028087615967),\n",
       " ('bermuda', 0.5716625452041626),\n",
       " ('apart', 0.524666428565979),\n",
       " ('wayside', 0.5025482177734375),\n",
       " ('loves', 0.4977417588233948),\n",
       " ('romantic', 0.47469043731689453),\n",
       " ('loved', 0.4740931987762451),\n",
       " ('flat', 0.46361207962036133),\n",
       " ('hate', 0.46154505014419556),\n",
       " ('friendship', 0.44779422879219055)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vecModel.wv.most_similar('love')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('stupidest', 0.7889276742935181),\n",
       " ('best', 0.7459392547607422),\n",
       " ('scariest', 0.7380221486091614),\n",
       " ('cheesiest', 0.7299014329910278),\n",
       " ('funniest', 0.7029139995574951),\n",
       " ('greatest', 0.676495373249054),\n",
       " ('poorest', 0.6579285264015198),\n",
       " ('weirdest', 0.6461619138717651),\n",
       " ('finest', 0.6428632736206055),\n",
       " ('dumbest', 0.6420060396194458)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vecModel.wv.similar_by_word(word='worst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getIndex(words, word2vecModel):\n",
    "    indices = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            indices.append(word2vecModel.wv.vocab[w].index)\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return np.array(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index = np.array([getIndex(sentence, word2vecModel) for sentence in train_processed_reviews])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_index = np.array([getIndex(sentence, word2vecModel) for sentence in test_processed_reviews])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Experiment 1: without any data cleaning training embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word2vecModel.wv.vocab) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42505"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max([len(x) for x in np.concatenate((train_index,test_index))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_length = sum([len(x) for x in np.concatenate((train_index,test_index))])/(len(train_index) + len(test_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_length = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length is 114.60574 and max length is 1337\n",
      "Setting max sequence length to be 500\n"
     ]
    }
   ],
   "source": [
    "print(\"Average length is {} and max length is {}\\nSetting max sequence length to be {}\".format(avg_length, max_length, max_sequence_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_vector = tokenizer.texts_to_sequences(texts=train_processed_reviews)\n",
    "train_sequences = pad_sequences(sequences=train_index, maxlen=max_sequence_length, padding='post', value=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_vector = tokenizer.texts_to_sequences(texts=test_processed_reviews)\n",
    "test_sequences = pad_sequences(sequences=test_index, maxlen=max_sequence_length, padding='post', value=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainY = np.zeros(len(training_labels))\n",
    "trainY[np.array(training_labels) > 6] = 1\n",
    "testY = np.zeros(len(test_labels))\n",
    "testY[np.array(test_labels) > 6] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label_one_hot = np_utils.to_categorical(np.array(training_labels)-1)\n",
    "test_label_one_hot = np_utils.to_categorical(np.array(test_labels)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_label_one_hot[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Masking(mask_value=-1.))\n",
    "model.add(word2vecModel.wv.get_keras_embedding(False))\n",
    "#model.add(LSTM(units=300,dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
    "model.add(LSTM(units=50,dropout=0.2, recurrent_dropout=0.2))\n",
    "#model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "chkpoint = ModelCheckpoint('weights-rnn-gensim-{epoch:02d}-{val_loss:.2f}.hdf5', mode='min', verbose=1, monitor='val_loss', save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/15\n",
      "25000/25000 [==============================] - 201s 8ms/step - loss: 0.6928 - acc: 0.5034 - val_loss: 0.6927 - val_acc: 0.5015\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.69272, saving model to weights-rnn-gensim-01-0.69.hdf5\n",
      "Epoch 2/15\n",
      "25000/25000 [==============================] - 197s 8ms/step - loss: 0.6923 - acc: 0.5025 - val_loss: 0.6926 - val_acc: 0.5014\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.69272 to 0.69257, saving model to weights-rnn-gensim-02-0.69.hdf5\n",
      "Epoch 3/15\n",
      "25000/25000 [==============================] - 198s 8ms/step - loss: 0.6917 - acc: 0.5008 - val_loss: 0.6924 - val_acc: 0.5020\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.69257 to 0.69241, saving model to weights-rnn-gensim-03-0.69.hdf5\n",
      "Epoch 4/15\n",
      "25000/25000 [==============================] - 196s 8ms/step - loss: 0.6913 - acc: 0.5071 - val_loss: 0.6951 - val_acc: 0.5015\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.69241\n",
      "Epoch 5/15\n",
      "25000/25000 [==============================] - 196s 8ms/step - loss: 0.6938 - acc: 0.5036 - val_loss: 0.6927 - val_acc: 0.5012\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.69241\n",
      "Epoch 6/15\n",
      "25000/25000 [==============================] - 195s 8ms/step - loss: 0.6922 - acc: 0.5014 - val_loss: 0.6927 - val_acc: 0.5003\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.69241\n",
      "Epoch 7/15\n",
      "25000/25000 [==============================] - 196s 8ms/step - loss: 0.6920 - acc: 0.5004 - val_loss: 0.6924 - val_acc: 0.5010\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.69241 to 0.69240, saving model to weights-rnn-gensim-07-0.69.hdf5\n",
      "Epoch 8/15\n",
      "25000/25000 [==============================] - 196s 8ms/step - loss: 0.6304 - acc: 0.6642 - val_loss: 0.5506 - val_acc: 0.7614\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.69240 to 0.55059, saving model to weights-rnn-gensim-08-0.55.hdf5\n",
      "Epoch 9/15\n",
      "25000/25000 [==============================] - 196s 8ms/step - loss: 0.5296 - acc: 0.7684 - val_loss: 0.4372 - val_acc: 0.8312\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.55059 to 0.43724, saving model to weights-rnn-gensim-09-0.44.hdf5\n",
      "Epoch 10/15\n",
      "25000/25000 [==============================] - 198s 8ms/step - loss: 0.4966 - acc: 0.7927 - val_loss: 0.4233 - val_acc: 0.8332\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.43724 to 0.42331, saving model to weights-rnn-gensim-10-0.42.hdf5\n",
      "Epoch 11/15\n",
      "25000/25000 [==============================] - 197s 8ms/step - loss: 0.4600 - acc: 0.8085 - val_loss: 0.4262 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.42331\n",
      "Epoch 12/15\n",
      "25000/25000 [==============================] - 198s 8ms/step - loss: 0.4462 - acc: 0.8168 - val_loss: 0.4008 - val_acc: 0.8467\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.42331 to 0.40082, saving model to weights-rnn-gensim-12-0.40.hdf5\n",
      "Epoch 13/15\n",
      "25000/25000 [==============================] - 196s 8ms/step - loss: 0.4762 - acc: 0.7928 - val_loss: 0.4080 - val_acc: 0.8444\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.40082\n",
      "Epoch 14/15\n",
      "25000/25000 [==============================] - 196s 8ms/step - loss: 0.5037 - acc: 0.7610 - val_loss: 0.5159 - val_acc: 0.7408\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.40082\n",
      "Epoch 15/15\n",
      "25000/25000 [==============================] - 196s 8ms/step - loss: 0.4675 - acc: 0.7994 - val_loss: 0.3885 - val_acc: 0.8535\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.40082 to 0.38850, saving model to weights-rnn-gensim-15-0.39.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5231e1ee48>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=train_sequences, y=trainY, epochs=15, batch_size=128, initial_epoch=0, validation_data=(test_sequences, testY), callbacks=[chkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = load_model('weights-rnn-gensim-15-0.39.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 16/30\n",
      "25000/25000 [==============================] - 259s 10ms/step - loss: 0.4381 - acc: 0.8232 - val_loss: 0.4093 - val_acc: 0.8406\n",
      "\n",
      "Epoch 00016: val_loss improved from inf to 0.40934, saving model to weights-rnn-gensim-16-0.41.hdf5\n",
      "Epoch 17/30\n",
      "25000/25000 [==============================] - 204s 8ms/step - loss: 0.4430 - acc: 0.8174 - val_loss: 0.3961 - val_acc: 0.8488\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.40934 to 0.39609, saving model to weights-rnn-gensim-17-0.40.hdf5\n",
      "Epoch 18/30\n",
      "25000/25000 [==============================] - 212s 8ms/step - loss: 0.4225 - acc: 0.8323 - val_loss: 0.4035 - val_acc: 0.8427\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.39609\n",
      "Epoch 19/30\n",
      "25000/25000 [==============================] - 214s 9ms/step - loss: 0.3976 - acc: 0.8431 - val_loss: 0.3567 - val_acc: 0.8657\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.39609 to 0.35673, saving model to weights-rnn-gensim-19-0.36.hdf5\n",
      "Epoch 20/30\n",
      "25000/25000 [==============================] - 198s 8ms/step - loss: 0.4043 - acc: 0.8438 - val_loss: 0.4102 - val_acc: 0.8399\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.35673\n",
      "Epoch 21/30\n",
      "25000/25000 [==============================] - 200s 8ms/step - loss: 0.4385 - acc: 0.8204 - val_loss: 0.4080 - val_acc: 0.8382\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.35673\n",
      "Epoch 22/30\n",
      "25000/25000 [==============================] - 198s 8ms/step - loss: 0.4134 - acc: 0.8391 - val_loss: 0.3560 - val_acc: 0.8708\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.35673 to 0.35602, saving model to weights-rnn-gensim-22-0.36.hdf5\n",
      "Epoch 23/30\n",
      "25000/25000 [==============================] - 200s 8ms/step - loss: 0.3944 - acc: 0.8481 - val_loss: 0.3582 - val_acc: 0.8653\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.35602\n",
      "Epoch 24/30\n",
      "25000/25000 [==============================] - 199s 8ms/step - loss: 0.3802 - acc: 0.8536 - val_loss: 0.3502 - val_acc: 0.8739\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.35602 to 0.35018, saving model to weights-rnn-gensim-24-0.35.hdf5\n",
      "Epoch 25/30\n",
      "25000/25000 [==============================] - 199s 8ms/step - loss: 0.3671 - acc: 0.8620 - val_loss: 0.3377 - val_acc: 0.8768\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.35018 to 0.33769, saving model to weights-rnn-gensim-25-0.34.hdf5\n",
      "Epoch 26/30\n",
      "25000/25000 [==============================] - 200s 8ms/step - loss: 0.3734 - acc: 0.8580 - val_loss: 0.3547 - val_acc: 0.8682\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.33769\n",
      "Epoch 27/30\n",
      "25000/25000 [==============================] - 196s 8ms/step - loss: 0.3607 - acc: 0.8606 - val_loss: 0.3313 - val_acc: 0.8763\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.33769 to 0.33131, saving model to weights-rnn-gensim-27-0.33.hdf5\n",
      "Epoch 28/30\n",
      "25000/25000 [==============================] - 193s 8ms/step - loss: 0.3400 - acc: 0.8680 - val_loss: 0.3351 - val_acc: 0.8746\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.33131\n",
      "Epoch 29/30\n",
      "25000/25000 [==============================] - 199s 8ms/step - loss: 0.3469 - acc: 0.8690 - val_loss: 0.3563 - val_acc: 0.8694\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.33131\n",
      "Epoch 30/30\n",
      "25000/25000 [==============================] - 195s 8ms/step - loss: 0.3479 - acc: 0.8696 - val_loss: 0.3451 - val_acc: 0.8690\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.33131\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f030d7bb908>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=train_sequences, y=trainY, epochs=30, batch_size=128, initial_epoch=15, validation_data=(test_sequences, testY), callbacks=[chkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 31/40\n",
      "25000/25000 [==============================] - 193s 8ms/step - loss: 0.3412 - acc: 0.8711 - val_loss: 0.3297 - val_acc: 0.8767\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.33131 to 0.32965, saving model to weights-rnn-gensim-31-0.33.hdf5\n",
      "Epoch 32/40\n",
      "25000/25000 [==============================] - 192s 8ms/step - loss: 0.3375 - acc: 0.8681 - val_loss: 0.3101 - val_acc: 0.8797\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.32965 to 0.31007, saving model to weights-rnn-gensim-32-0.31.hdf5\n",
      "Epoch 33/40\n",
      "25000/25000 [==============================] - 192s 8ms/step - loss: 0.3182 - acc: 0.8752 - val_loss: 0.3043 - val_acc: 0.8842\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.31007 to 0.30433, saving model to weights-rnn-gensim-33-0.30.hdf5\n",
      "Epoch 34/40\n",
      "25000/25000 [==============================] - 192s 8ms/step - loss: 0.3259 - acc: 0.8752 - val_loss: 0.3016 - val_acc: 0.8860\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.30433 to 0.30158, saving model to weights-rnn-gensim-34-0.30.hdf5\n",
      "Epoch 35/40\n",
      "25000/25000 [==============================] - 192s 8ms/step - loss: 0.3202 - acc: 0.8776 - val_loss: 0.3068 - val_acc: 0.8860\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.30158\n",
      "Epoch 36/40\n",
      "25000/25000 [==============================] - 193s 8ms/step - loss: 0.3189 - acc: 0.8803 - val_loss: 0.2968 - val_acc: 0.8865\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.30158 to 0.29683, saving model to weights-rnn-gensim-36-0.30.hdf5\n",
      "Epoch 37/40\n",
      "25000/25000 [==============================] - 193s 8ms/step - loss: 0.3063 - acc: 0.8792 - val_loss: 0.2822 - val_acc: 0.8892\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.29683 to 0.28225, saving model to weights-rnn-gensim-37-0.28.hdf5\n",
      "Epoch 38/40\n",
      "25000/25000 [==============================] - 196s 8ms/step - loss: 0.2949 - acc: 0.8856 - val_loss: 0.2830 - val_acc: 0.8896\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.28225\n",
      "Epoch 39/40\n",
      "25000/25000 [==============================] - 194s 8ms/step - loss: 0.3094 - acc: 0.8774 - val_loss: 0.2968 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.28225\n",
      "Epoch 40/40\n",
      "25000/25000 [==============================] - 233s 9ms/step - loss: 0.3078 - acc: 0.8718 - val_loss: 0.2842 - val_acc: 0.8825\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.28225\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f030d920780>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=train_sequences, y=trainY, epochs=40, batch_size=128, initial_epoch=30, validation_data=(test_sequences, testY), callbacks=[chkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vecModel.save('gensim_imbd_word_embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('rnn-gensim-embedding-epoch-40.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
